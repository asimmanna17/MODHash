{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# torch.cuda.set_device(0)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.manual_seed(3407)\n",
    "torch.cuda.manual_seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.dirname(os.getcwd())\n",
    "print(base_path)\n",
    "sys.path.insert(0,base_path)\n",
    "from AlexNet import Encoder, Modality, Organ, Disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP(q_name, sorted_pool):\n",
    "    #choose modality\n",
    "    ret_classes = [sorted_pool[i][0].split(\"_\")[1] for i in range(len(sorted_pool))]\n",
    "    q_class = q_name.split(\"_\")[1]\n",
    "    #print(q_class)\n",
    "    #print(ret_classes)\n",
    "    initlist = [int(q_class == i) for i in ret_classes]\n",
    "    #print(initlist)\n",
    "    den = np.sum(initlist)\n",
    "    #print(den)\n",
    "    if den == 0:\n",
    "        return 0\n",
    "    x = 0\n",
    "    preclist = [0]*len(initlist)\n",
    "    for idx, pts in enumerate(initlist):\n",
    "        x += pts #rel(n)\n",
    "        preclist[idx] = x/(idx+1) #rel(n)/k\n",
    "    #print(preclist)\n",
    "    num = np.dot(preclist, initlist)\n",
    "    #print(num)\n",
    "    #print(num/den)\n",
    "    return num/den\n",
    "\n",
    "def nDCG(relevance_list, ideal_relevance_list):\n",
    "    \"\"\"\n",
    "    Calculates the Normalized Discounted Cumulative Gain (NDCG) of a ranked list of documents.\n",
    "\n",
    "    Args:\n",
    "        relevance_list: A list of relevance scores for the documents in the ranked list.\n",
    "        ideal_relevance_list: A list of ideal relevance scores for the documents in the ranked list.\n",
    "\n",
    "    Returns:\n",
    "        The NDCG of the ranked list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the cumulative gain of the ranked list.\n",
    "    cumulative_gain = 0.0\n",
    "    for id, relevance in enumerate(relevance_list):\n",
    "        numerator = 2**relevance - 1\n",
    "        # add 1 because python 0-index\n",
    "        denominator =  np.log2(id + 2) \n",
    "        cumulative_gain += numerator/denominator\n",
    "    #print(cumulative_gain)\n",
    "\n",
    "    # Calculate the ideal cumulative gain.\n",
    "    ideal_cumulative_gain = 0.0\n",
    "    for id, ideal_relevance in enumerate(ideal_relevance_list):\n",
    "        numerator = 2**ideal_relevance - 1\n",
    "        # add 1 because python 0-index\n",
    "        denominator =  np.log2(id + 2)\n",
    "        #print('1', numerator) \n",
    "        ideal_cumulative_gain += numerator/denominator\n",
    "    #print(ideal_cumulative_gain)\n",
    "\n",
    "    # Normalize the cumulative gain by the ideal cumulative gain.\n",
    "    if ideal_cumulative_gain==0:\n",
    "        return 0\n",
    "    else:\n",
    "        ndcg = cumulative_gain / ideal_cumulative_gain\n",
    "        return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevenceClasses(sorted_pool,q_name):\n",
    "    value = []\n",
    "    q_labels = q_name.split(\"_\")[1] # for modality\n",
    "    #print(q_labels)\n",
    "    for i in range(len(sorted_pool)):\n",
    "        #print(q_labels)\n",
    "        sorted_pool_labels = sorted_pool[i][0].split(\"_\")[1] # for modality\n",
    "        #print(sorted_pool_labels)\n",
    "        common_labels = len(set(sorted_pool_labels).intersection(q_labels))\n",
    "        value.append(common_labels)  \n",
    "        #print(value)\n",
    "    value2 = sorted(value, reverse=True)\n",
    "    return value, value2\n",
    "\n",
    "def hammingDistance(h1, h2):\n",
    "    hash_code = h1.shape[1]\n",
    "    h1norm = torch.div(h1, torch.norm(h1, p=2))\n",
    "    h2norm = torch.div(h2, torch.norm(h2, p=2))\n",
    "    distH = torch.pow(torch.norm(h1norm - h2norm, p=2), 2) * hash_code / 4\n",
    "    return distH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcode = 16\n",
    "ocode = 16\n",
    "dcode = 16\n",
    "hash_code = mcode+ ocode+ dcode\n",
    "#model load######################\n",
    "nModality = 5\n",
    "nOrgan = 4\n",
    "nDisease = 13\n",
    "encoder = Encoder()\n",
    "mClassifier = Modality(nModality, mcode)\n",
    "oClassifier = Organ(nOrgan, ocode)\n",
    "dClassifier = Disease(nDisease, dcode)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    mClassifier.cuda()\n",
    "    oClassifier.cuda()\n",
    "    dClassifier.cuda()\n",
    "\n",
    "dataStorePath = os.path.join(base_path, 'models')\n",
    "\n",
    "encoder_path = os.path.join(dataStorePath, f'encoder_{mcode}_{ocode}_{dcode}.pkl')\n",
    "\n",
    "modality_path = os.path.join(dataStorePath, f'modality_{mcode}_{ocode}_{dcode}.pkl')\n",
    "\n",
    "organ_path = os.path.join(dataStorePath, f'organ_{mcode}_{ocode}_{dcode}.pkl')\n",
    "\n",
    "disease_path = os.path.join(dataStorePath, f'disease_{mcode}_{ocode}_{dcode}.pkl')\n",
    "\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "mClassifier.load_state_dict(torch.load(modality_path))\n",
    "oClassifier.load_state_dict(torch.load(organ_path))\n",
    "dClassifier.load_state_dict(torch.load(disease_path))\n",
    "\n",
    "print(encoder_path)\n",
    "galleryfolderpath = os.path.join(base_path, 'data', 'gallery/') # specify gallery path for characrter specific experiment \n",
    "queryfolderpath = os.path.join(base_path, 'data', 'query/') # specify gallery path for characrter specific experiment \n",
    "\n",
    "gallery_files = os.listdir(galleryfolderpath)\n",
    "gallery_files = random.sample(gallery_files, len(gallery_files))\n",
    "query_files = os.listdir(queryfolderpath)\n",
    "query_files = random.sample(query_files, len(query_files))\n",
    "print(len(gallery_files))\n",
    "querynumber = len((query_files))\n",
    "print(querynumber)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery = {}\n",
    "print(\"\\n\\n Building Gallery .... \\n\")\n",
    "with torch.no_grad():\n",
    "    # Process each gallery image\n",
    "    for img in gallery_files:\n",
    "        image_path = os.path.join(galleryfolderpath, img)\n",
    "\n",
    "        # Load and transform the image\n",
    "        image = np.load(image_path)\n",
    "        # transfer to one channel\n",
    "        if len(image.shape)!= 2:\n",
    "            image = np.mean(image,axis=-1)\n",
    "\n",
    "        image = Image.fromarray(image)\n",
    "        tensor_image = transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "        # Pass the tensor through the  model\n",
    "        x_e = encoder(tensor_image)\n",
    "        _, mh1 = mClassifier(x_e)\n",
    "        '''_, oh1 = oClassifier(x_e)\n",
    "        _, dh1 = dClassifier(x_e)\n",
    "        h = torch.cat((mh1, oh1, dh1), dim = 1)\n",
    "        h = torch.sign(h)'''\n",
    "        h = torch.sign(mh1)\n",
    "        gallery[img] = h # Store the result in the gallery dictionary\n",
    "        \n",
    "        # Clean up\n",
    "        del tensor_image\n",
    "    print(\"\\n Building Complete. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "q_prec_10 = 0\n",
    "q_prec_100 = 0\n",
    "q_prec_1000 = 0\n",
    "\n",
    "nDCG_list_10 = []\n",
    "nDCG_list_100 = []\n",
    "nDCG_list_1000 = []\n",
    "\n",
    "#print(len(qNimage[0:100]))\n",
    "for q_name in query_files:\n",
    "    count = count+1\n",
    "    query_image_path = os.path.join(queryfolderpath, q_name)\n",
    "    # Load and transform the image\n",
    "    query_image = np.load(query_image_path)\n",
    "    # transfer to one channel\n",
    "    if len(query_image.shape)!= 2:\n",
    "        query_image = np.mean(query_image,axis=-1)\n",
    "    query_image = Image.fromarray(query_image)\n",
    "    query_tensor_image = transform(query_image).unsqueeze(0).cuda()\n",
    "\n",
    "    # Pass the tensor through the model\n",
    "    q_x_e = encoder(query_tensor_image)\n",
    "    _, q_mh1 = mClassifier(q_x_e)\n",
    "    '''_, q_oh1 = oClassifier(q_x_e)\n",
    "    _, q_dh1 = dClassifier(q_x_e)\n",
    "    h_q = torch.cat((q_mh1, q_oh1, q_dh1), dim = 1)'''\n",
    "    h_q = torch.sign(q_mh1)\n",
    "    dist = {}\n",
    "    for key, h1 in gallery.items():\n",
    "        dist[key] = hammingDistance(h1, h_q)\n",
    "\n",
    "    print(count)   \n",
    "    ### images with sorted distance \n",
    "    sorted_pool_10 = sorted(dist.items(), key=operator.itemgetter(1))[0:10]\n",
    "    \n",
    "    sorted_pool_100 = sorted(dist.items(), key=operator.itemgetter(1))[0:100]\n",
    "\n",
    "    #### mean average precision\n",
    "    q_prec_10 += mAP(q_name, sorted_pool_10)\n",
    "    q_prec_100 += mAP(q_name, sorted_pool_100)\n",
    "\n",
    "    ### nDCG\n",
    "    r_i_10, sorted_r_i_10 = relevenceClasses(sorted_pool_10, q_name)\n",
    "    r_i_100, sorted_r_i_100 = relevenceClasses(sorted_pool_100, q_name)\n",
    "    #print(r_i, sorted_r_i)\n",
    "\n",
    "    nDCG_value_10 = nDCG(r_i_10, sorted_r_i_10)\n",
    "    nDCG_list_10.append(nDCG_value_10)\n",
    "\n",
    "    nDCG_value_100 = nDCG(r_i_100, sorted_r_i_100)\n",
    "    nDCG_list_100.append(nDCG_value_100)\n",
    "\n",
    "\n",
    "    if count % 10 == 0:\n",
    "        print(\"mAP@10 :\", q_prec_10/count)\n",
    "        print(\"mAP@100 :\", q_prec_100/count)\n",
    "        print('-------------------------------')\n",
    "        print('nDCG@10:', sum(nDCG_list_10)/len(nDCG_list_10))\n",
    "        print('nDCG@100:', sum(nDCG_list_100)/len(nDCG_list_100))\n",
    "\n",
    "\n",
    "print('-----------------------------------------------')       \n",
    "print(\"mAP@10 :\", q_prec_10/count)\n",
    "print(\"mAP@100 :\", q_prec_100/count)\n",
    "print('-------------------------------')\n",
    "print('nDCG@10:', sum(nDCG_list_10)/len(nDCG_list_10))\n",
    "print('nDCG@100:', sum(nDCG_list_100)/len(nDCG_list_100))\n",
    "print('Result for modality')\n",
    "print(f'encoder_{mcode}_{ocode}_{dcode}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asimenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
